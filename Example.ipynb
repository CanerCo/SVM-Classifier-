{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "sys.path.append('./code')\n",
    "from preprocessing import *\n",
    "from tweets_embedding import *\n",
    "from frnn_owa_eval import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload data and perform preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datasets\n",
    "file_train_path = '../data/hateval2019_en_train.csv'\n",
    "file_dev_path = '../data/hateval2019_en_dev.csv'\n",
    "file_test_path = '../data/hateval2019_en_test.csv'\n",
    "Raw_tweet = 'text'\n",
    "col = ['id', Raw_tweet, 'HS']\n",
    "sep = ','\n",
    "\n",
    "# Upload as a DataFrames (preprocessing is biult-in functions)\n",
    "# If we have test dataset\n",
    "train, dev, data, test = upload_datasets(file_train_path, file_dev_path, file_test_path, col, sep)\n",
    "\n",
    "# If we have only train dataset - use this line instead\n",
    "#data = transform_data(file_train_path, col, sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply embedding methods on the train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roBERTa\n",
    "\n",
    "# preload components to save time\n",
    "# path to the pre-loaded roBERTa model \n",
    "MODEL_path_roberta = r\"..\\model\\twitter-roberta-base-hate\"\n",
    "# upload tokenizer and model\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(MODEL_path_roberta)\n",
    "model_roberta = TFAutoModel.from_pretrained(MODEL_path_roberta)\n",
    "\n",
    "# Apply on 3 tweet preprocessing options\n",
    "for column in tqdm([Raw_tweet, 'Cleaned_tweet', 'Cleaned_tweet_wt_stopwords']):\n",
    "    data['Vector_roBERTa_'+column] = data[column].apply(lambda x: \n",
    "                                            get_vector_roberta(x, tokenizer_roberta, model_roberta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT\n",
    "\n",
    "# preload components to save time\n",
    "# load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# upload BERT model \n",
    "model_bert = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "\n",
    "# Apply on 3 tweet preprocessing options\n",
    "for column in tqdm([Raw_tweet, 'Cleaned_tweet', 'Cleaned_tweet_wt_stopwords']):\n",
    "    data['Vector_BERT_'+column] = data['Cleaned_tweet'].apply(lambda x: \n",
    "                                                      get_vector_bert(x, tokenizer_bert, model_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sBERT\n",
    "\n",
    "# preload components to save time\n",
    "# upload Sentence-BERT model from the 'sentence_transformers' package \n",
    "model_sbert = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# Apply on 3 tweet preprocessing options\n",
    "for column in tqdm([Raw_tweet, 'Cleaned_tweet', 'Cleaned_tweet_wt_stopwords']):\n",
    "    data['Vector_sBERT_'+column] = data[Raw_tweet].apply(lambda x: get_vector_sbert(x, model_sbert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepMoji\n",
    "  \n",
    "# If dataset is big, it's better to split it\n",
    "for column in [Raw_tweet, 'Cleaned_tweet', 'Cleaned_tweet_wt_stopwords']:\n",
    "    data['Vector_DeepMoji_'+column] = None\n",
    "    for i in range(20):\n",
    "        ind = list(range(i*int(len(data[column])/20),(i+1)*int(len(data[column])/20)))\n",
    "        data['Vector_DeepMoji_'+column].iloc[ind] = get_vectors_deepmoji(data[column].iloc[ind]) \n",
    "\n",
    "# If dataset is small, you can use this:\n",
    "#for column in [Raw_tweet, 'Cleaned_tweet','Cleaned_tweet_wt_stopwords']:\n",
    "    #data['Vector_DeepMoji_'+column] = get_vectors_deepmoji(data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE \n",
    "\n",
    "# preload components to save time\n",
    "# upload the big Universal Sentence Encoder model from HTTPS domain \n",
    "model_use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "\n",
    "# Apply on 3 tweet preprocessing options\n",
    "for column in [Raw_tweet, 'Cleaned_tweet', 'Cleaned_tweet_wt_stopwords']:\n",
    "    data['Vector_USE_'+column] = data['Cleaned_tweet'].apply(lambda x: get_vector_use(x, model_use))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "\n",
    "# preload components to save time\n",
    "# path to the pre-loaded Word2Vec model \n",
    "w2v_path = '../model/GoogleNews-vectors-negative300.bin'\n",
    "# upload model\n",
    "model_w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "\n",
    "# Apply on 3 tweet preprocessing options\n",
    "for column in tqdm([Raw_tweet, 'Cleaned_tweet', 'Cleaned_tweet_wt_stopwords']):\n",
    "    data['Vector_Word2Vec_'+column] = data[Raw_tweet].apply(lambda x: get_vector_w2v(x, model_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings in a file\n",
    "data.to_csv('../data/hs_train_embeddings.csv')\n",
    "\n",
    "# To upload saved embeddings \n",
    "#data = pd.read_csv('../data/hs_train_embeddings.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply cross-validation for OWA-FRNN to evaluate k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for roBERTa\n",
    "\n",
    "f1_raw_roBERTa = []\n",
    "f1_clean_roBERTa = []\n",
    "f1_wtstop_roBERTa = []\n",
    "for k in tqdm([11, 13, 15, 17, 19, 21, 23, 25, 27, 29]):\n",
    "    f1_raw_roBERTa.append(cross_validation_ensemble_owa(data, ['Vector_roBERTa_text'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))\n",
    "    f1_clean_roBERTa.append(cross_validation_ensemble_owa(data, ['Vector_roBERTa_Cleaned_tweet'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))\n",
    "    f1_wtstop_roBERTa.append(cross_validation_ensemble_owa(data, ['Vector_roBERTa_Cleaned_tweet_wt_stopwords'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=16.3037567870835, pvalue=3.171119831143775e-12)\n",
      "Ttest_indResult(statistic=126.65411058202282, pvalue=5.181029547995751e-28)\n",
      "Ttest_indResult(statistic=129.0724725673128, pvalue=3.687333754309249e-28)\n"
     ]
    }
   ],
   "source": [
    "#if 0.05 > p-value - arrays are different\n",
    "\n",
    "print(ttest_ind(f1_raw_roBERTa, f1_clean_roBERTa))\n",
    "print(ttest_ind(f1_clean_roBERTa, f1_wtstop_roBERTa))\n",
    "print(ttest_ind(f1_raw_roBERTa, f1_wtstop_roBERTa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for BERT\n",
    "\n",
    "f1_raw_BERT = []\n",
    "f1_clean_BERT = []\n",
    "f1_wtstop_BERT = []\n",
    "for k in tqdm([11, 13, 15, 17, 19, 21, 23, 25, 27, 29]):\n",
    "    f1_raw_BERT.append(cross_validation_ensemble_owa(data, ['Vector_BERT_text'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))\n",
    "    f1_clean_BERT.append(cross_validation_ensemble_owa(data, ['Vector_BERT_Cleaned_tweet'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))\n",
    "    f1_wtstop_BERT.append(cross_validation_ensemble_owa(data, ['Vector_BERT_Cleaned_tweet_wt_stopwords'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-16.416286874851846, pvalue=2.8226833111845612e-12)\n",
      "Ttest_indResult(statistic=50.04418390655184, pvalue=8.928317165178167e-21)\n",
      "Ttest_indResult(statistic=33.062355197035764, pvalue=1.4357237896271443e-17)\n"
     ]
    }
   ],
   "source": [
    "print(ttest_ind(f1_raw_BERT, f1_clean_BERT))\n",
    "print(ttest_ind(f1_clean_BERT, f1_wtstop_BERT))\n",
    "print(ttest_ind(f1_raw_BERT, f1_wtstop_BERT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for sBERT\n",
    "\n",
    "f1_raw_sBERT = []\n",
    "f1_clean_sBERT = []\n",
    "f1_wtstop_sBERT = []\n",
    "for k in tqdm([11, 13, 15, 17, 19, 21, 23, 25, 27, 29]):\n",
    "    f1_raw_sBERT.append(cross_validation_ensemble_owa(data, ['Vector_sBERT_text'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))\n",
    "    f1_clean_sBERT.append(cross_validation_ensemble_owa(data, ['Vector_sBERT_Cleaned_tweet'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))\n",
    "    f1_wtstop_sBERT.append(cross_validation_ensemble_owa(data, ['Vector_sBERT_Cleaned_tweet_wt_stopwords'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=3.4837515074149, pvalue=0.002650634653343464)\n",
      "Ttest_indResult(statistic=22.53379083616941, pvalue=1.217166840178357e-14)\n",
      "Ttest_indResult(statistic=22.470330376354124, pvalue=1.2784313992113216e-14)\n"
     ]
    }
   ],
   "source": [
    "print(ttest_ind(f1_raw_sBERT, f1_clean_sBERT))\n",
    "print(ttest_ind(f1_clean_sBERT, f1_wtstop_sBERT))\n",
    "print(ttest_ind(f1_raw_sBERT, f1_wtstop_sBERT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for DeepMoji\n",
    "\n",
    "f1_raw_DeepMoji = []\n",
    "f1_clean_DeepMoji = []\n",
    "f1_wtstop_DeepMoji = []\n",
    "for k in tqdm([11, 13, 15, 17, 19, 21, 23, 25, 27, 29]):\n",
    "    f1_raw_DeepMoji.append(cross_validation_ensemble_owa(data, ['Vector_DeepMoji_text'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))\n",
    "    f1_clean_DeepMoji.append(cross_validation_ensemble_owa(data, ['Vector_DeepMoji_Cleaned_tweet'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))\n",
    "    f1_wtstop_DeepMoji.append(cross_validation_ensemble_owa(data, ['Vector_DeepMoji_Cleaned_tweet_wt_stopwords'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=40.393241550449225, pvalue=4.087237312094933e-19)\n",
      "Ttest_indResult(statistic=-21.899299312650374, pvalue=2.0005806130929803e-14)\n",
      "Ttest_indResult(statistic=27.066850571796405, pvalue=4.917158474993862e-16)\n"
     ]
    }
   ],
   "source": [
    "print(ttest_ind(f1_raw_DeepMoji, f1_clean_DeepMoji))\n",
    "print(ttest_ind(f1_clean_DeepMoji, f1_wtstop_DeepMoji))\n",
    "print(ttest_ind(f1_raw_DeepMoji, f1_wtstop_DeepMoji))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for USE\n",
    "\n",
    "f1_raw_USE = []\n",
    "f1_clean_USE = []\n",
    "f1_wtstop_USE = []\n",
    "for k in tqdm([11, 13, 15, 17, 19, 21, 23, 25, 27, 29]):\n",
    "    f1_raw_USE.append(cross_validation_ensemble_owa(data, ['Vector_USE_text'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))\n",
    "    f1_clean_USE.append(cross_validation_ensemble_owa(data, ['Vector_USE_Cleaned_tweet'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))\n",
    "    f1_wtstop_USE.append(cross_validation_ensemble_owa(data, ['Vector_USE_Cleaned_tweet_wt_stopwords'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-12.555781559784352, pvalue=2.426472410269981e-10)\n",
      "Ttest_indResult(statistic=10.024645089049283, pvalue=8.598847759513292e-09)\n",
      "Ttest_indResult(statistic=-0.8566987347867746, pvalue=0.4028731773532028)\n"
     ]
    }
   ],
   "source": [
    "print(ttest_ind(f1_raw_USE, f1_clean_USE))\n",
    "print(ttest_ind(f1_clean_USE, f1_wtstop_USE))\n",
    "print(ttest_ind(f1_raw_USE, f1_wtstop_USE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for Word2Vec\n",
    "\n",
    "f1_raw_W2V = []\n",
    "f1_clean_W2V = []\n",
    "f1_wtstop_W2V = []\n",
    "for k in tqdm([11, 13, 15, 17, 19, 21, 23, 25, 27, 29]):\n",
    "    f1_raw_W2V.append(cross_validation_ensemble_owa(data, ['Vector_Word2Vec_text'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))\n",
    "    f1_clean_W2V.append(cross_validation_ensemble_owa(data, ['Vector_Word2Vec_Cleaned_tweet'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))\n",
    "    f1_wtstop_W2V.append(cross_validation_ensemble_owa(data, ['Vector_Word2Vec_Cleaned_tweet_wt_stopwords'], 'HS', K_fold, [k], additive(), additive(), 'labels', 'f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=0.843432546225684, pvalue=0.4100554944124666)\n",
      "Ttest_indResult(statistic=-33.024896297917174, pvalue=1.4648602590988157e-17)\n",
      "Ttest_indResult(statistic=-30.150039658636025, pvalue=7.340637716543324e-17)\n"
     ]
    }
   ],
   "source": [
    "print(ttest_ind(f1_raw_W2V, f1_clean_W2V))\n",
    "print(ttest_ind(f1_clean_W2V, f1_wtstop_W2V))\n",
    "print(ttest_ind(f1_raw_W2V, f1_wtstop_W2V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the best preprocessing, the most efficient k, and F1 score that they provide**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest F1 score:  0.8769230027547492  with k =  25\n",
      "The highest F1 score:  0.7172900711451752  with k =  15\n",
      "The highest F1 score:  0.7063688555139183  with k =  15\n",
      "The highest F1 score:  0.6223832079832569  with k =  19\n",
      "The highest F1 score:  0.7037900857065156  with k =  13\n",
      "The highest F1 score:  0.6700108681026531  with k =  13\n"
     ]
    }
   ],
   "source": [
    "ks = [11, 13, 15, 17, 19, 21, 23, 25, 27, 29]\n",
    "\n",
    "for array in [f1_raw_roBERTa, f1_clean_BERT, f1_raw_sBERT, f1_raw_DeepMoji, f1_clean_USE, f1_wtstop_W2V]:\n",
    "    print('The highest F1 score: ', max(array), ' with k = ', ks[array.index(max(array))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble of the best models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7500831390090896\n"
     ]
    }
   ],
   "source": [
    "# labels - it means we use the same weights for all models\n",
    "\n",
    "cross_validation_ensemble_owa(data, ['Vector_roBERTa_text', 'Vector_BERT_Cleaned_tweet', \n",
    "                            'Vector_sBERT_text', 'Vector_DeepMoji_text', 'Vector_USE_Cleaned_tweet', \n",
    "                            'Vector_Word2Vec_Cleaned_tweet_wt_stopwords'], 'HS', K_fold, \n",
    "                              [25, 15, 15, 19, 13, 13], \n",
    "                              additive(), additive(), 'labels', 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8435949487244606\n"
     ]
    }
   ],
   "source": [
    "# confident scores - we use CS as weight for models' outputs\n",
    "\n",
    "cross_validation_ensemble_owa(data, ['Vector_roBERTa_text', 'Vector_BERT_Cleaned_tweet', \n",
    "                            'Vector_sBERT_text', 'Vector_DeepMoji_text', 'Vector_USE_Cleaned_tweet', \n",
    "                            'Vector_Word2Vec_Cleaned_tweet_wt_stopwords'], 'HS', K_fold, \n",
    "                              [25, 15, 15, 19, 13, 13], \n",
    "                              additive(), additive(), 'conf_scores', 'f1')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.8116510478070585\n",
      "0.2\n",
      "0.8073159290890268\n",
      "0.30000000000000004\n",
      "0.8076055021832225\n",
      "0.4\n",
      "0.8106237180682454\n",
      "0.5\n",
      "0.8089246279070123\n",
      "0.6\n",
      "0.8079684318718336\n",
      "0.7000000000000001\n",
      "0.8092404184071709\n",
      "0.8\n",
      "0.8115789041556499\n",
      "0.9\n",
      "0.8103005890308277\n"
     ]
    }
   ],
   "source": [
    "# tune alpha parameter for confidence scores approach\n",
    "\n",
    "for alpha in np.arange(0.1, 1, 0.1):\n",
    "    print(alpha)\n",
    "    print(cross_validation_ensemble_owa(data, ['Vector_roBERTa_text', \n",
    "                            'Vector_BERT_Cleaned_tweet', 'Vector_sBERT_text', \n",
    "                            'Vector_DeepMoji_text', 'Vector_USE_Cleaned_tweet', \n",
    "                            'Vector_Word2Vec_Cleaned_tweet_wt_stopwords'], 'HS', 5, \n",
    "                              [25, 19, 15, 15, 13, 13], \n",
    "                              additive(), additive(), 'conf_scores', 'f1', alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest F1-score = 0.8116510478070585 was provided by alpha = 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the best models setup with grid search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors =  ['Vector_roBERTa_text', 'Vector_BERT_Cleaned_tweet', 'Vector_sBERT_text', \n",
    "            'Vector_DeepMoji_text', 'Vector_USE_Cleaned_tweet', \n",
    "              'Vector_Word2Vec_Cleaned_tweet_wt_stopwords']\n",
    "\n",
    "size = [0, 1, 2, 3, 4, 5]\n",
    "neighbours = [25, 19, 15, 15, 13, 13]\n",
    "anger_names = []\n",
    "anger_means = []\n",
    "\n",
    "for L in range(len(vectors)+1):\n",
    "    for subset in itertools.combinations(size, L):\n",
    "        anger_names.append([i.split('_')[1] for i in [vectors[k] for k in subset]])\n",
    "        res = cross_validation_ensemble_owa(data, [vectors[k] for k in subset], 'HS', K_fold, \n",
    "                                            [neighbours[k] for k in subset], \n",
    "                                      additive(), additive(), 'conf_scores', 'f1', 0.1)\n",
    "        anger_means.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest F1 score: 0.8765144484070471 with models: roBERTa\n"
     ]
    }
   ],
   "source": [
    "print('The highest F1 score: ', max(anger_means), ' with models:', \n",
    "      anger_names[anger_means.index(max(anger_means))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply on the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply roBERTa on the test data\n",
    "\n",
    "test['Vector_roBERTa_text'] = test['text'].apply(lambda x: get_vector_roberta(x, tokenizer_roberta, model_roberta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predicted labels\n",
    "\n",
    "test_labels = test_ensemble_labels(data, data['HS'], test, ['Vector_roBERTa_text'], [25], additive(), additive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1-score for irony test data:  0.5351496563781046\n"
     ]
    }
   ],
   "source": [
    "p_hs, r_hs, f1_hs, support = precision_recall_fscore_support(test['HS'], test_labels, average = \"macro\")\n",
    "print(\"Test F1-score for irony test data: \", f1_hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leaderboard:**\n",
    "\n",
    "F1-score (averaged):\n",
    "0.651\n",
    "0.571\n",
    "0.546\n",
    "0.535\n",
    "0.519\n",
    "\n",
    "We got 0.5351 - it will be 4th or 5th place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore explainability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes of wrong predicted test instances\n",
    "\n",
    "error = [i for i in range(len(test['HS'])) if test['HS'][i] != test_labels[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAKE UP AMERICA. We cannot continue to allow illegal aliens to stay in County. They are a real and present danger to LEGAL AMERICAN CITIZENS. #BuildThatWall #EndCatchAndReleash #DefundSantuaryCities\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Correct sample\n",
    "\n",
    "i = 304 # not in error list\n",
    "print(hs_test['text'].iloc[i])\n",
    "print(hs_test['HS'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Migrants go Home. https://t.co/bUCcoB8480',\n",
       "  '@SenatorLankford #NoDaca #NoAmnesty #NoAnchorBabies 2 illegals do not make a legal #BuildTheWall',\n",
       "  'Illegal Criminals EVERYWHERE #BuildThatWall !!',\n",
       "  '@KamalaHarris Again going on record that you care more for illegals than US Citizens. Theyre here illegally! #NoDACA #NoAmnesty',\n",
       "  'Since 911 63,000 Americans have been killed by Illegal Aliens  #Trump #MAGA #RedNationRising  #Immigration’ #SaturdayMorning https://t.co/qqmOnUMtZh',\n",
       "  '@SpeakerRyan #BuildThatWall and deport, deport, deport. We have a right to live in an illegal-less and safe country.',\n",
       "  'Dr. Vliet on #FOXNews 7/12/2014 @JudgeJeanine Nothing has changed! #Illegals bring #USA diseases!#BuildThatWall ASAP.@DrLee4America',\n",
       "  \"@narendramodi Pl don't wait for SC intervention.All 126.90Crs Indians r with u. Pl deport all Rohingyas&amp;other illegal migrants.Big threat\",\n",
       "  'Rep. Biggs pitches plan to fund border wall by penalizing countries of illegal immigrants https://t.co/fQg0Vys4SK'],\n",
       " [1, 1, 1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore its train neighbours and their classes\n",
    "\n",
    "test_vector = hs_test['Vector_roBERTa'].iloc[i]\n",
    "get_neigbours(test_vector, hs_train, 'Vector_roBERTa', 9, 'text', 'HS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Last Refuge has a fantastic collection of reports on a business model that profits from illegal immigration. #UniParty #RobbingUsBlind #EndChainMigration#tcot #ccot #pjnet #qanon\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Sample of wrong classified instance\n",
    "\n",
    "i = 321 # is in error list \n",
    "print(hs_test['text'].iloc[i])\n",
    "print(hs_test['HS'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The Truth about #Immigration https://t.co/nKPVzuTB2M',\n",
       "  'Forced migration history: https://t.co/mw7ApfnFQZ',\n",
       "  '@john_tatnell @PimlicoPlumbers @RevMcCafferty @afneil Mass immigration for one',\n",
       "  'Victor D. Hanson: The 4 Groups that Benefit from IllegalÂ\\xa0Immigration https://t.co/kd5TuU0W1n https://t.co/ISmJOJd2S4',\n",
       "  '@TOOEdit Bishop Schneider: mass migration a plan to undermine identity https://t.co/0rnRAL1Kp6',\n",
       "  'NPR is trying to group legal and illegal immigration\"Jeff Sessions is threatening immigrants rights in... https://t.co/cvnMVppGlO',\n",
       "  'The Last Refuge has a fantastic collection of reports on a business model that profits from illegal immigration. #UniParty #RobbingUsBlind #EndChainMigration#tcot #ccot #pjnet #qanon',\n",
       "  \"Trump's slippery slope. #ImmigrantChildren #Resistance #NurembergTrials @realDonaldTrump @TheJusticeDept @StephenMillerAL @SecNielsen @PressSec @SenateGOP @HouseGOP https://t.co/oVBp1abcWp\",\n",
       "  'Italy agrees to accept migrant arrivals https://t.co/hq9wLooqef'],\n",
       " [0, 0, 0, 0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore its train neighbours and their classes\n",
    "\n",
    "test_vector = hs_test['Vector_roBERTa'].iloc[i]\n",
    "get_neigbours(test_vector, hs_train, 'Vector_roBERTa', 9, 'text', 'HS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data reference:**\n",
    "    \n",
    "*V. Basile, C. Bosco, E. Fersini, N. Debora, V. Patti, F. M. R. Pardo, P. Rosso, M. Sanguinetti, et al., Semeval-2019 task 5: Multilingual detection of hate speech against immigrants and women in twitter, in: 13th International Workshop on Semantic Evaluation, 2019, pp. 54–63*       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
